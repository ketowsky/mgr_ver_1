\documentclass[a4paper,12pt,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{polski}
\usepackage{helvet}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{color}
\usepackage{listings}
\usepackage{leading}
\usepackage{lipsum}
\usepackage{float}
\usepackage{diagbox}
\usepackage{amsfonts}
\usepackage{makecell}
\usepackage{url}
\usepackage[none]{hyphenat} 
\usepackage{hyperref}
\usepackage{chngpage}
\usepackage{graphicx}% http://ctan.org/pkg/graphicx
\usepackage{array}% http://ctan.org/pkg/array
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{tocloft}
\renewcommand\cftsecleader{\cftdotfill{\cftdotsep}}
\usepackage{fancyhdr}
\usepackage[top=2.5cm,bottom=2.5cm,inner=3.5cm,outer=2.5cm]{geometry}
\leading{20pt}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\pagestyle{fancy}
\fancyhf{}
\fancyhead{}
\fancyhead[RO,LE]{\nouppercase{\leftmark}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\begin{document}
% ===============  STRONA TYTULOWA PRACY MAGISTERSKIEJKIEJ ==============
\thispagestyle{empty}
%% ------------------------ NAGLOWEK STRONY ------------------------------
\includegraphics[height=37.5mm]{dudel.jpg}\\
\rule{30mm}{0pt}
{\large \textsf{Wydział Fizyki i Informatyki Stosowanej}}\\
\rule{\textwidth}{3pt}\\
\rule[2ex]
{\textwidth}{1pt}\\

\vspace{1ex}

\begin{center}
{\LARGE \bf \textsf{Praca magisterska}}\\

\vspace{10ex}

% --------------------------- IMIE I NAZWISKO ----------------------------
{\bf \Large \textsf{Kamil Madej}}\\
\vspace{3ex}
{\sf\small kierunek studiów:} {\bf\small \textsf{Informatyka Stosowana}}\\
\vspace{1.5ex}
{\sf\small specjalność:} {\bf\small \textsf{Grafika Komputerowa i Przetwarzanie Obrazów}}\\
\vspace{10ex}
%% ------------------------ TYTUL PRACY ----------------------------------
{\bf \huge \textsf{Sposoby zwiększania bezpieczeństwa systemu informatycznego przy pomocy aplikacji do zarządzania zasobami systemów informatycznych}}\\
\vspace{10ex}
%% ------------------------ OPIEKUN PRACY --------------------------------
{\Large Opiekun: \bf \textsf{dr inż. Antoni Dydejczyk}}\\
\vspace{10ex}
{\large \bf \textsf{Kraków, czerwiec 2018}}
\end{center}
%% ===============  STRONA TYTULOWA PRACY MAGISTERSKIEJKIEJ ==============

\newpage
%% ============= TYL STRONY TYTULOWEJ PRACY MAGISTERSKIEJKIEJ ============
{\sf Oświadczam, świadomy odpowiedzialności karnej za poświadczenie nieprawdy, że niniejszą pracę dyplomową wykonałem osobiście i samodzielnie i  nie korzystałem ze źródeł innych niż wymienione w pracy.}
\vspace{14ex}
\begin{flushright}
\begin{tabular}{lr}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ &
................................................................. \\
~ & {\sf (czytelny podpis)}\\
\end{tabular}
\end{flushright}

\newpage
%% ============================== DEDYKACJA =============================
\mbox{}
\vfill{%
\begin{flushright}
    TODO
\end{flushright}
}

\mbox{}

\newpage
%% ===============  TEMATYKA PRACY MAGISTERSKIEJKIEJ ===================
\rightline{Kraków, czerwiec 2018}
\begin{center}
{\bf Tematyka pracy magisterskiej i praktyki dyplomowej
Kamila Madeja,
studenta V roku studiów kierunku informatyka stosowana, specjalności grafika komputerowa i przetwarzanie obrazów}\\
\end{center}

Temat pracy magisterskiej:
{\bf Sposoby zwiększania bezpieczeństwa systemu informatycznego przy pomocy aplikacji do zarządzania zasobami systemów informatycznych}\\

\begin{tabular}{rl}

Opiekun pracy:                  & dr inż. Antoni Dydejczyk\\
Recenzenci pracy:               & dr inż. TODO\\
Miejsce praktyki dyplomowej:    & WFiIS AGH, Kraków\\
\end{tabular}
\vspace{3ex}
\begin{center}
{\bf Program pracy magisterskiej i praktyki dyplomowej}
\end{center}

\begin{enumerate}
\item Omówienie realizacji pracy magisterskiej z opiekunem oraz przedstawicielem firmy IBM.
\item Zebranie i opracowanie literatury dotyczącej tematu pracy.
\item Praktyka dyplomowa:
\begin{itemize}
\item Zapoznanie się z ideą Software Asset Management
\item Zapoznanie się z tematyką podatności systemów informatycznych
\item Analiza danych wejściowych
\end{itemize}
\item Opracowanie algorytmu umożliwiającego wykrywanie podatności w systemie
\item Przeprowadzenie testów, analiza wyników
\item Porównanie otrzymanych wyników z próbami przeprowadzonymi przez IBM
\item Analiza wyników, ich omówienie i zatwierdzenie przez opiekuna
\item Opracowanie redakcyjne pracy
\end{enumerate}

\noindent
Termin oddania w dziekanacie: TODO czerwca 2018\\[1cm]

\begin{center}
\begin{tabular}{lcr}
.......................................................... & ~~~ &
.......................................................... \\
(podpis kierownika katedry) & & (podpis opiekuna) \\
\end{tabular}
\end{center}

\newpage
%% =========================  RECENZJE ==================================
\thispagestyle{empty}
\mbox{}

\null\newpage
Merytoryczna ocena pracy przez opiekuna: TODO

\newpage
\thispagestyle{empty}
\mbox{}

\null\newpage
Merytoryczna ocena pracy przez recenzenta: TODO

\newpage
%% =========================  SPIS TRESCI ================================
\thispagestyle{empty}
\mbox{}
\null\newpage
\tableofcontents


\newpage
%% ============================ WSTEP ====================================
\section*{Wstęp}
\addcontentsline{toc}{section}{Wstęp}
\markboth{Wstęp}{}
\paragraph{}
    Software Asset Management jest pojęciem ściśle związanym z dynamicznie rozwijającym się rynkiem oprogramowania. Pojawiające się różne rodzaje licencji określające sposób, w jaki można korzystać z poszczególnych rozwiązań, stworzyły przestrzeń dla firm takich, jak IBM, na stworzenie aplikacji ułatwiającej zarządzanie licencjami w systemach informatycznych przedsiębiorstw. W dzisiejszych realiach poważne przedsiębiorstwa muszą działać transparentnie, nie mogą sobie bowiem pozwolić na oskarżenia o nadużywanie lub nieprzestrzeganie wymogów licencyjnych. Wiąże się to nie tylko z kosztami ponoszonymi wskutek naliczanych kar, ale też ze stratami wizerunkowymi.

    \paragraph{}
    Każdego dnia pojawiają się nowe podatności i zagrożenia, na jakie wystawione są systemy informatyczne. Twórcom oprogramowania zależy, aby ich produkt był jak najbardziej na nie odporny. Wymusza to publikowanie nowych wersji oprogramowania, w którym poprawiono błędy i załatano luki umożliwiające niepożądane działanie systemu.      

    \paragraph{}
    Niniejsza praca jest próbą połączenia powyższych pojęć w jednym narzędziu. Wykorzystując informacje gromadzone w aplikacji IBM Big Fix Inventory, można informować użytkownika o tym, że w jego systemie informatycznym zainstalowane jest oprogramowanie, w którym wykryta została podatność. Właściciel systemu na podstawie informacji o rodzaju podatności i o stopniu zagrożenia z nią związanego, może zdecydować jakie akcje należy podjąć, by zapewnić stabilność i bezpieczeństwo systemu.

    \paragraph{}
    Algorytm opracowany w ramach pracy umożliwia wykrycie w systemie informatycznym wersji oprogramowania znajdujących się na liście obciążonych podatnościami, publikowanej przez amerykańską agencję NIST. Jego działanie polega na dopasowaniu informacji o oprogramowaniu zainstalowanym i używanym w systemie, zebranych przez IBM Big Fix Inventory, z informacjami zebranymi przez NIST.    
    
\paragraph{}



\newpage
%% =========================== CEL PRACY =================================
\section*{Cel pracy}
\addcontentsline{toc}{section}{Cel pracy}
\markboth{Cel pracy}{}
\paragraph{}
Niniejsza praca stanowi badanie możliwości wykorzystania różnych algorytmów porównania tekstu oraz ich połączeń w celu wykrywania oprogramowania, które może powodować podatność systemu informatycznego na zagrożenia. W ramach pracy zostaną wykonane następujące czynności:
\begin{itemize}
\item Analiza poszczególnych algorytmów pod kątem danych wejściowych. 
\item Na podstawie powyższej analizy zostanie opracowany algorytm pozwalający na połączenie dostępnych informacji o podatnościach występujących oprogramowaniu informatycznym oraz informacji o oprogramowaniu zainstalowanym w badanym systemie.
\item Implementacja algorytmu w języku Python
\item Przeprowadzenie testów skuteczności algorytmu
\item Analiza otrzymanych wyników
\end{itemize}
\paragraph{}
Podczas opracowywania algorytmu szczególna uwaga zwrócona będzie w kierunku skuteczności wykrycia zagrożonej wersji oprogramowania zainstalowanej w systemie informatycznym monitorowanym przez narzędzie IBM Big Fix Inventory. Efektem pracy będą przedstawione wnioski dotyczące efektywności poszczególnych algorytmów porównania tekstu oraz ich połączeń opracowanych w ramach pracy. Wnioski zostaną wyciągnięte na podstawie wyników testów poszczególnych rozwiązań.
\newpage
\thispagestyle{empty}
\mbox{}

\newpage
%% =========================== ROZDZIAL 1 ================================
\section{Klasyfikacja}
\subsection{Wstęp}
\paragraph{}
Eksploracja danych zajmuje się analizą dużych zbiorów danych dotyczących różnych zjawisk. Mogą to być dane finansowe, wyniki obserwacji fizyczny czy dane pacjentów. Danych najczęściej jest zbyt dużo aby mógł je przeanalizować człowiek, dlatego też istnieje zapotrzebowanie na automatyczne algorytmy operujące na całym zbiorze danych. Zbiór zwykle przedstawiany jest w postaci macierzy której kolumnami są cechy opisujące zjawisko, a wiersze to kolejne obserwacje. Analizowane dane mogą mieć następujące typy:
\begin{itemize}
    \item Liczbowe - Numeryczny wynik jakiegoś pomiaru, na przykład wzrost, waga
    \item Kategoryczne - Wartość z ograniczonego zbioru, na przykład rozróżnienie kobiet i mężczyzn, czy grupy krwi
    \item Tekstowe - Bloki tekstu opisujące zjawisko, na przykład treść wiadomości e-mail 
\end{itemize}
\paragraph{}
Jedną z podstawowych procedur Eksploracji danych jest klasyfikacja, polegająca na dopasowywaniu nowych obserwacji do jednej z kategorii(klas). Nieprzypisana obserwacja powinna zostać przyporządkowana do klasy w której znajdują się  elementy najbardziej do niej podobne. Miarą podobieństwa może być odległość, pochodzenie z tego samego rozkładu lub inne bardziej złożone miary. Procedura ta zakłada istnienie zbioru trenującego już podzielonego na klasy, który będzie służył za przykład na podstawie którego dopasowane zostaną nowe rekordy.  Przykładem klasyfikacji może być  ocena czy pacjent cierpi na jakąś chorobę, czy określenie gatunku ziarna na podstawie jego wymiarów i wagi.  Klasyfikacja jest przykładem uczenia nadzorowanego, procedura uczy się na podstawie dostarczonych do niej przykładów. Przykładem uczenia nienadzorowanego jest klasteryzacja, w której celem jest podział zbioru danych na grupy bez żadnych informacji podanych z zewnątrz.
\newpage
\subsection{Dostępne rozwiązania}
\subsubsection{Klasyfikator K-najbliższych sąsiadów}
\paragraph{}
Klasyfikator K-Najbliższych sąsiadów opiera się na założeniu że obserwacje ze zbioru uczącego znajdujące się najbliżej dostarczają istotnych informacji na temat klasy do której należy klasyfikowana obserwacja. Na potrzeby tego klasyfikatora należy zdefiniować pojęcie odległości w danym zbiorze danych. W przypadku danych liczbowych odległość może zostać zdefiniowana przy pomocy metryki Euklidesowej, i taka tez jest najczęściej wykorzystywana w praktyce. Inne typy danych nie mogą być bezpośrednio wykorzystywane do obliczania odległości. Dane tekstowe należy przekształcić na dane numeryczne lub kategoryczne przy pomocy procedur eksploracji tekstu. Cechy kategoryczne o więcej niż dwóch możliwych wartościach można przykładowo umieścić na sympleksie o odpowiedniej wymiarowości, tak aby odległości wszystkich wyborów były równe. Przebieg algorytmu \cite{knn}:
\begin{itemize}
\item Znajdź K elementów o najmniejszej odległości od klasyfikowanego
\item Przypisz element do klasy z której pochodzi najwięcej sąsiadów
\end{itemize} 
\paragraph{}
Dobór wartości K jest bardzo istotny dla działania algorytmu. Małe K powoduje że algorytm jest bardzo narażony na wpływ elementów nietypowych, a granicach klas pojawi się bardzo wiele źle zaklasyfikowanych elementów.  Duża wartość K może spowodować że mało liczne klasy i skupienia będą ignorowane, co również negatywnie wpłynie na jakość klasyfikacji \cite{knn}. Zaletą algorytmu jest jego prostota, jednakże jest on zwykle mniej skuteczny od bardziej zaawansowanych algorytmów.
\newpage
\subsubsection{Drzewa decyzyjne}
\paragraph{}
Drzewa decyzyjne to zbiór algorytmów których celem jest zbudowanie modelu w postaci drzewa umożliwiającego podjęcie decyzji na temat przynależności elementu do danej klasy. Algorytmu budowy drzew decyzyjnych polegają na rekursywnym podziale zbioru danych \cite{tree}. Celem działania drzewa decyzyjnego jest podział przestrzeni możliwych danych wejściowych na takie części aby każda z nich zawierała wyłącznie elementy jednej klasy. W węzłach drzewa umieszczane są kryteria na podstawie których dzielona jest przestrzeń danych wejściowych. 
\paragraph{}
Istnieje wiele algorytmów opartych o drzewa decyzyjne, klasycznym przykładem jest algorytm Iterative Dichotomiser 3 (ID3) w którym wybór decyzji jaka zostanie dodana do drzewa kierowany jest kryterium maksymalizacji zysku informacyjnego. Zysk informacyjny to różnica pomiędzy entropią zbioru danych przed i po dodaniu decyzji. W zbiorze uporządkowanym entropia powinna wynosić 0 ( w każdym zbiorze będą znajdować się tylko elementy jednej klasy).
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{dudel.jpg}
    \caption{Struktura drzewa decyzyjnego w algorytmie ID3}
\end{figure}


\subsubsection{Maszyna wektorów nośnych}
\paragraph{}
Maszyna wektorów nośnych (ang. support vector machines, SVM), to metoda klasyfikacji binarnej której celem jest wyznaczenie hiperpłaszczyzny oddzielającej od siebie elementy dwóch klas \cite{svm}. Niestety, nie wszystkie zbiory danych można znaleźć granicę pomiędzy klasami danych, najczęściej w oryginalnej reprezentacji dane w pewnych obszarach mieszają się ze sobą. Aby rozwiązać ten problem w metodzie SVM stosuje się nieliniową transformacje zbioru danych w przestrzeń o znacznie wyższej wymiarowości w której zbiory danych staną się separowalne liniowo. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{dudel.jpg}
    \caption{Liniowy podział zbioru danych.}
\end{figure}
\paragraph{}
Algorytm SVM polega na znalezieniu takiej linii rozdzielającej punkty aby odległość od najbliższego punktu każdej z klas była jak największa, tak by zapewnić jak największy margines klasyfikacji \cite{svm}. Taki wybór linii rozdzielającej znacznie poprawia właściwości uogólniające sieci. SVM działa tyko dla przypadku dwóch klas, aby umożliwić klasyfikacja większej ilości klas należy utworzyć wiele instancji SVM z których każda będzie podejmować decyzję czy dany punkt należy do danej klasy czy do pozostałych klas. 

\newpage
\subsubsection{Sieci neuronowe}
\paragraph{}
Sztuczne sieci neuronowe mogą być wykorzystywane do aproksymacji dowolnej funkcji, mogą więc również zostać użyte do klasyfikacji. Sztuczna sieć składa się z wielu połączonych ze sobą modeli neuronów. Neurony stanowią zwykle matematyczny model biologicznych neuronów. Pojedynczy neuron ma za zadanie zsumować sygnały ze wszystkich swoich wejść z zadanymi wagami, a następnie dokonać pogowania \cite{nn}. Sieć neuronowa składa się z kilku warstw  połączonych neuronów, zazwyczaj istnieją co najmniej dwie warstwy, warstwa wejściowa gdzie istnieje dokładnie jeden neuron dla każdej cechy zbioru danych, i warstwa wyjściowa z liczbą neuronów odpowiadającą złożoności problemu. W przypadku klasyfikacji, w warstwie wyjściowej zwykle znajduje się dokładnie tyle neuronów ile jest klas, gdzie każdy z nich daje binarną odpowiedź o przynależności do jednej z klas, bądź stosuje się dodatkową agregującą warstwę które przyporządkowuje badany element do klasy na podstawie wyjść poprzedniej warstwy. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{dudel.jpg}
    \caption{Uproszczony schemat sieci neuronowej.}
\end{figure}
\paragraph{}
Proces uczenia sieci neuronowej polega na dobraniu takich wag aby sieć realizowała pożądane zadanie. Wprowadza się pojęcie funkcji celu której wartość będzie optymalizowana, zwykle poprzez kwadrat odległości wyjścia sieci neuronowej od wartości faktycznej. Do optymalizacji wykorzystuje się techniki propagacji wstecznej i algorytmy spadku gradientowego \cite{nn}.
Innym rodzajem sieci neuronowej jest sieć probabilistyczna, która zostanie opisana w kolejnym rozdziale.
\newpage
\thispagestyle{empty}
\mbox{}

\newpage
%% =========================== ROZDZIAL 2 ================================
\section{Opis teoretyczny badanego rozwiązania}
\subsection{Jądrowy estymator gęstości}
\paragraph{}
Rozkład gęstości prawdopodobieństwa to funkcja opisująca prawdopodobieństwo wystąpienia elementu w zależności od wartości wszystkich współrzędnych \cite{kde}. 
\begin{equation} f:\mathbb{R}^n\rightarrow [0,\infty )
\end{equation}
Znajomość tej funkcji dostarcza wielu informacji na temat zbioru danych, na jej podstawie opiera się wiele algorytmów statystycznych, między innymi ten wykorzystany w pracy. Funkcja ta jednak nigdy nie jest dana w sposób jawny dla zbioru danych, dlatego należy ją estymować na podstawie dostępnych wartości.
\paragraph{}
Do przybliżania wartości funkcji rozkładu gęstości prawdopodobieństwa wykorzystano metodę estymatorów jądrowych. Gęstość prawdopodobieństwa często jest funkcją o wielu lokalnych maksimach i minimach, dlatego też nie można jej efektywnie estymować  żadnym standardowym rozkładem prawdopodobieństwa, dlatego też wybrano metodę nieparametryczną ( nie zakładającą przynależności do danego rozkładu). Metoda estymatorów jądrowych polega na sumowaniu funkcji jądrowych (ang. kernel functions) przesuniętych do każdego z elementów zbioru trenującego. Jednowymiarowy estymator gęstości opisany jest więc wzorem \cite{kde}:
\begin{equation} \hat{f}(x)=\frac{1}{mh}\sum_{i=1}^{m}K\left ( \frac{x-x_{i}}{h} \right )
\end{equation}
Gdzie m to liczność zbioru zdanych, K to funkcja estymatora jądrowego, a h to parametr wygładzania. Jądro musi być funkcją symetryczną z maksimum globalnym w zerze, a jego całka musi wynosić jeden \cite{kde2}. Istnieje wiele funkcji spełniających te warunki, dobór jądra zostanie omówiony w podrozdziale 2.3. Na rysunku 4 widać  funkcję rozkładu gęstości prawdopodobieństwa, która przyjmuje wysokie wartości w tych przedziałach w których znajduje się duża liczba elementów, i bardzo niskie w obszarach w których elementy nie występują. Wartość tej funkcji można interpretować jako prawdopodobieństwo wystąpienia konkretnego zestawienia wartości wszystkich cech zbioru danych.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{dudel.jpg}
    \caption{Jądrowy estymator gęstości jednowymiarowej funkcji}
\end{figure}


\newpage
Dla przypadku zmiennej losowej wielowymiarowej funkcja estymatora jądrowego opisywana jest przez wzór \cite{kde2}: \begin{equation}
\hat{f}(x,H)=\frac{1}{m}\sum_{i=1}^{m}\left | H \right |^{-1/2} K\left ( H^{-1/2}x \right )\label{eq:1}
\end{equation}
Gdzie $H$ to macierz współczynników wygładzania a $K$ to jądro wielowymiarowe.
Istnieją dwie metody generacji jąder wielowymiarowych z jąder jednowymiarowych, jądro produktywne i jądro radialne. Jądro produktywne to jądro wielowymiarowe utworzone jako iloraz jąder jednowymiarowych \cite{kde2}:
\begin{equation} K^n(x)=\prod_{i=1}^{n} K^1(x_{i})
\end{equation}

Alternatywnym podejściem jest stosowanie jądra radialnego \cite{kde2}:

\begin{equation} K^n(x)=c_{K,n}K^1((x^Tx)^{1/2})
\end{equation}
W niniejszej pracy wykorzystano podejście produktywne.
\subsection{Probabilistyczna sieć neuronowa}
\paragraph{}
Probabilistyczna sieć neuronowa to struktura wykorzystująca funkcję rozkładu gęstości prawdopodobieństwa  do celu klasyfikacji. Struktura sieci przedstawiona jest na rysunku 5. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{dudel.jpg}
    \caption{Struktura probabilistycznej sieci neuronowej}
\end{figure}
\paragraph{}
Pierwsza warstwa to warstwa wejściowa, każdy neuron warstwy wejściowej reprezentuje jeden z atrybutów zbioru danych. Wartości atrybutów klasyfikowanego elementu przekazywane są na wejścia neuronów wejściowych. W drugiej warstwie ( warstwa wzorów) obliczane są wartości funkcji jądrowej przesuniętej do punktu ze zbioru uczącego w badanym punkcie dla każdego elementu trenującego. Warstwa ta zawiera wszystkie informacje ze zbioru trenującego. Powstałe wartości są sumowane i skalowane w warstwie sumacyjnej. Sumowanie przeprowadzane jest po wszystkich elementach klasy, więc neuronów w warstwie sumacyjnej jest dokładnie tyle ile klas w zbiorze danych. Wyjścia warstwy sumacyjnej są odpowiednikami obliczenia wartości jądrowego estymatora gęstości dla każdej z klas w badanym punkcie.   Ostatnia warstwa podejmuje decyzję o klasyfikacji. Wybrana zostaje klasa której wartość wyjścia warstwy sumacyjnej jest największa.
\paragraph{}
W przeciwieństwie do klasycznych sieci neuronowych, siec probabilistyczna nie wymaga procesu optymalizacji wag wszystkich połączeń. Proces uczenia PNN polega na budowie struktury i umieszczeniu w niej wszystkich elementów zbioru trenującego, oraz na odpowiednim dobraniu parametrów jądrowego estymatora gęstości, które zostaną opisane w kolejnych podrozdziałach. Wadą probabilistycznych sieci neuronowych jest jednakże wysoki czas klasyfikacji, który uwarunkowany jest umieszczeniem całego zbioru danych w strukturze sieci.
\subsection{Dobór jądra}
Funkcja jądrowa (ang. kernel) powinna spełniać poniższe warunki \cite{kde2}:
\begin{equation} \int K(x)dx=1
\end{equation}
\begin{equation} \int xK(x)dx=0
\end{equation}
\begin{equation} \int x^2K(x)dx<\inf
\end{equation}
\begin{equation}  K(x)>0
\end{equation}
\begin{equation}  K(x)=K(-x)
\end{equation}

Pierwsze wymaganie podyktowane jest chęcią modelowania funkcji gęstości prawdopodobieństwa, której całka również musi wynosić jeden. Pozostałe warunki wynikają ze specyfiki procesu estymacji, warto aby funkcja jądra była symetryczna aby nie kierować danych w jednym z kierunków.  Istnieje wiele funkcji spełniających powyższe warunki, nie można również stwierdzić które z okien daje najlepsze rezultaty. W tabeli 1 wypisano jądra badane w niniejszej pracy. 
\newpage
\begin{table}[h!]
  \centering
  \caption{Jądra}
  \begin{tabular}{ | c | m{6cm} | m{3cm} | }
    \hline
    Wykres & Wzór & Nazwa \\ \hline
    \begin{minipage}[c][7cm]{.4\textwidth}
      \includegraphics[width=\linewidth, height=60mm]{dudel.jpg}
    \end{minipage}
    &
     $ K(x)=1/2, |x|\leq1$
    & 
    Jednostajne
    \\ \hline
    \begin{minipage}[c][7cm]{.4\textwidth}
      \includegraphics[width=\linewidth, height=60mm]{dudel.jpg}
    \end{minipage}
    &
     $ K(x)=(1-|x|), |x|\leq1$
    & 
    Trójkątne
    \\ \hline
    \begin{minipage}[c][7cm]{.4\textwidth}
      \includegraphics[width=\linewidth, height=60mm]{dudel.jpg}
    \end{minipage}
    &
     $ K(x)=3/4(1-x^2), |x|\leq1$
    & 
    Epanecznikowa
    \\ \hline
    \end{tabular}
  \label{tbl:myLboro}
\end{table}
\begin{table}[h!]
  \centering
  \caption{Jądra cz.2}
  \begin{tabular}{ | c | m{6cm} | m{3cm} | }
    \hline
    Wykres & Wzór & Nazwa \\ \hline
    \begin{minipage}[c][7cm]{.4\textwidth}
      \includegraphics[width=\linewidth, height=60mm]{dudel.jpg}
    \end{minipage}
    &
    $ K(x)=15/16(1-x^2)^2, |x|\leq1$
    & 
    Dwówagowe
    \\ \hline
    \begin{minipage}[c][7cm]{.4\textwidth}
      \includegraphics[width=\linewidth, height=60mm]{dudel.jpg}
    \end{minipage}
    &
     $ K(x)=1/\sqrt{2\pi}e^{-1/2u^2} $
    & 
    Normalne
    \\ \hline
    \begin{minipage}[c][7cm]{.4\textwidth}
      \includegraphics[width=\linewidth, height=60mm]{dudel.jpg}
    \end{minipage}
    &
     $ K(x)=2/\pi(x^2+1)^2 $
    & 
    Cauchiego
    \\ \hline
    \end{tabular}
  \label{tbl:myLboro}
\end{table}
\clearpage
\paragraph{}
Jądro Epanecznikowa jest optymalne pod względem błędu średniokwadratowego estymacji \cite{epa}, jednakże wykorzystywana jest znaczna ilość innych okien, przede wszystkim dlatego że teoretyczna różnica dokładności nie jest duża, a często wybiera się okna o dobrych właściwościach matematycznych, ułatwiając tym samym przeprowadzanie obliczeń. Z tego właśnie powodu często wykorzystywane są jądra Cauchiego i normalne. Można też zauważyć że istnieją dwa typy jąder: 
\begin{itemize}
\item O skończonej szerokości: Jednostajne, Trójkątne, Epanecznikowa, Dwówagowe
\item O nieskończonej szerokości: Normalne, Cauchiego
\end{itemize}
Różnice pomiędzy tymi typami okien  oraz wyniki uzyskane z różnymi oknami zostaną omówione w części praktycznej pracy.
\subsection{Parametr wygładzania}
\paragraph{}
Bardzo istotny w procesie estymacji jądrowej jest parametr wygładzania. Parametr ten kontroluje szerokość jądra, duże wartości powodują mocne wygładzenie estymatora rozkładu, natomiast małe wartości prowadzą do bardzo dużej zmienności estymatora. We wzorze \ref{eq:1} symbol H oznacza macierz symetryczną parametrów wygładzania, można analizować kilka stopni dokładności doboru parametru wygładzania:
\begin{itemize}
\item Parametr wygładzania taki sam dla wszystkich wymiarów i klas - H to macierz powstała przez przemnożenie skalara przez macierz jednostkową, ta sama dla każdej klasy.
\item Parametr wygładzania taki sam dla wszystkich klas, zmienny dla wymiarów - H to macierz diagonalna, ta sama dla każdej klasy.
\item Parametr wygładzania taki sam dla wszystkich wymiarów, zmienny dla klas - H to macierz powstała przez przemnożenie skalara przez macierz jednostkową, różna dla każdej klasy.
\item Parametr wygładzania zmienny dla wymiarów i klas - H to macierz diagonalna, różna dla każdej klasy.
\end{itemize}

W pracy zbadane zostaną wszystkie wymienione podejścia. 

\subsection{Dobór parametru wygładzania metodą plug-in}
\paragraph{}
Istnieje wiele metod doboru parametru wygładzania, w przypadku jądra radialnego najczęściej wykorzystywana jest metoda kros-walidacji, natomiast w przypadku podejścia produktywnego najczęściej wykorzystuje się metodę podstawiania (ang. plug-in) \cite{kde}. W niniejszej pracy wykorzystana i omówiona zostanie metoda plug-in. 
Jakość estymacji oceniana jest miarą całkowego błędu średnio kwadratowego MISE:
\begin{equation}  MISE(\hat{f})=E\left(\int_{R^n}\left[\hat{f}(x)-f(x)\right]^2 dx\right)
\end{equation}
Gdzie E oznacza wartość oczekiwaną. Celem metody plug-in jest znalezienie h minimalizującego powyższe wyrażenie. 

Można pokazać że wartością minimalizującą powyższe wyrażenie dla przypadku jednowymiarowego z błędem zmieżającym do zera wraz ze wzrostem rozmiaru zbioru danych jest \cite{kde2}:
\begin{equation}  h_{MISE}=\left[ \frac{R(K)}{mP(K)^2\Psi_{4}}\right]^\frac{1}{5} \label{eqn2}
\end{equation}
Gdzie:
\begin{equation}  R(K)=\int_{R^n}K(x)^2dx
\end{equation}
\begin{equation}  P(K)=\int_{R^n}x^TxK(x)dx
\end{equation}
\begin{equation} \Psi_{4}=\int^{\infty}_{-\infty }f^{(4)}(x)f(x)dx
\end{equation}

\paragraph{}
Jedynym nieznanym elementem równania \ref{eqn2}  $\Psi_{4}$. W metodzie plug-in do znalezienia tej zmiennej wykorzystuję się proces estymacji jądrowej pochodnej rozkładu prawdopodobieństwa. Do estymacji pochodnej użyty może zostać dowolny inny estymator jądrowy $K_{1}$ i dowolny inny parametr wygładzania g. Proces wyznaczania estymatora jądrowego wymaga znajomości jego drugiej pochodnej, natomiast proces wyznaczania estymatora jądrowego drugiej pochodnej wymaga znajomości pochodnej czwartego rzędu. W ogólności wyznaczenie h dla pochodnej rzędu N wymaga znajomości pochodnej rzędu N+2. Aby uzyskać dokładną wartość $h_{MISE}$ należało by przeanalizować nieskończoną ilość pochodnych, dlatego też stosuje się pewne przybliżenie. Proces podstawiania kończy się po skończonej ilości iteracji ( w niniejszej pracy 3), a do estymacji pochodnej najwyższego rzędu wykorzystuje się przybliżenie nie oparte o estymatory jądrowe. 
Estymator $\Psi_{r}$ zdefiniowany jest następująco \cite{kde}:
\begin{equation} \hat{\Psi}_{r}(g)=\frac{1}{m^2g^{r+1}}\sum^m_{i=1}\sum^m_{j=1}K_{1}^{(r)}(\frac{x_{i}-x_{j}}{g})
\end{equation}
Po zastosowaniu estymatora wzór na $h_{MISE}$  wygląda następująco:
\begin{equation}  h_{MISE}=\left[ \frac{R(K)}{mP(K)^2 \hat{\Psi}_{4}(g_{4})}\right]^\frac{1}{5} 
\end{equation}
Wartość g stanowi parametr wygładzania estymatora pochodnej rozkładu gęstości prawdopodobieństwa, i można go obliczyć ze wzoru: 
\begin{equation}  g_{r}=\left[ \frac{2K^{(r)}(0)}{mP(K)\hat{\Psi}_{r+2}(g_{r+2})}\right]^\frac{1}{r+3} 
\end{equation}

Jak widzimy, równania wykazują zależność rekurencyjną. W przypadku algorytmu trzeciego stopnia dokonujemy obliczeń $\Psi_{4}$, $\Psi_{6}$ i $\Psi_{8}$ według podanych wzorów, natomiast $\Psi_{10}$ potrzebne do obliczenia $\Psi_{8}$ zostaje estymowane przy pomocy wzoru normalnej skali (ang. normal scale) \cite{kde2}:
\begin{equation} \hat{\Psi}^{NS}_{r}=\left[\frac{-1^{r/2}r!}{(2\hat{\sigma})^{r+1}(r/2)!\pi^{1/2}}\right]^\frac{1}{9} 
\end{equation}
\subsection{Modyfikacja parametru wygładzania}
\paragraph{}
Modyfikacja parametru wygładzania to proces uzależnienia stopnia wygładzania od lokalnej gęstości występowania elementów w estymatorze jądrowym. Algorytm modyfikacjie przebiega następujaco \cite{kde}:
\begin{itemize}
\item Obliczenie estymatora gęstości $\hat{f}_{*}$ nie używając modyfikacji parametru wygładzania, wykorzystując parametry wygładzania obliczone na przykład przy pomocy metody plug-in
\item Obliczenie parametru modyfikacji $s_{i}$ dla każdego elementu zbioru danych $x_{i}$ według wzoru:
\begin{equation} s_{i}=\left(\frac{\hat{f}_{*}(x_{i})}{s}\right)^{-c} 
\end{equation}
Gdzie s to średnia geometryczna wartości estymatora $\hat{f}_{*}$  dla wszystkich elementów zbioru danych, a c to większa lub równa od zera zmienna oznaczająca siłę modyfikacji.
\item Obliczenie estymatora gęstości wykorzystując wyznaczony parametr modyfikacji korzystając ze wzoru:
\begin{equation}
\hat{f}(x)=\frac{1}{mh^n}\sum_{i=1}^{m}\frac{1}{s^{n}_{i}}K\left ( \frac{x-x_{i}}{hs^{n}_{i}} \right )
\end{equation}
\end{itemize}
 \paragraph{}
Modyfikacja parametru wygładzania ma korzystny wpływ z dwóch przyczyn, po pierwsze zwiększone wygładzanie w rejonach niskiej gęstości znacznie poprawia odporność klasyfikatora na elementy nietypowe. Rekord bardzo oddalony od jakiegokolwiek innego zostanie bardzo mocno wygładzony, dzięki czemu jego wpływ na wynik będzie minimalny.  W rejonach wysokiej gęstości nie potrzebne jest duże wygładzanie, a każde wygładzanie prowadzi do utraty informacji (wysokie częstotliwości w funkcji rozkładu gęstości zostają odfiltrowane), dlatego też zmniejszenie stopnia wygładzania w rejonach wysokiej gęstości pozytywnie wpływa na jakość estymacji. 
 Wpływ stosowania modyfikacji wygładzania zostanie zbadany w niniejszej pracy.
\subsection{Flower Pollination Algorithm}
\paragraph{}
W niniejszej pracy badany będzie wpływ zastosowania algorytmu optymalizacyjnego do doboru parametru wygładzania w probabilistycznej sieci neuronowej. Algorytm jaki został wybrany to oparty o schematy biologiczne Flower Pollination Algorithm (FPA, pol. Algortym zapylania kwiatów). FPA to algorytm metaheourystyczny, nie istnieją mocne teoretyczne podstawy dla jego skuteczności, proces optymalizacji przebiegać będzie w ten sam sposób niezależnie od optymalizowanej funkcji.  Inspiracją do stworzenia tego algorytmu jest proces zapylania kwiatów w którym możemy wyróżnić dwa oddzielne pod procesy \cite{FPA}:
\begin{itemize}
\item Zapylanie lokalne, odbywające się w obrębie jednego kwiatu bądź w obrębie kwiatów tej samej rośliny lub poprzez czynniki abiotyczne jak na przykład wiatr. 
\item Zapylanie globalne, odbywające się pod wpływem działania organizmów zewnętrznych jak na przykład pszczoły, które mogą pokonywać duże dystanse pomiędzy roślinami
\end{itemize}
\paragraph{}
Obydwa procesy wykorzystane zostają w algorytmie, aby je wykorzystać należy przedstawić je w postaci równań aktualizacji pozycji wektora rozwiązań. W przypadku zapylania globalnego aktualizacja przebiega według wzoru \cite{FPA}:
\begin{equation}
x^{t+1}_i=x^t_i + \gamma L(\lambda)(g_*-x^t_i)\label{global}
\end{equation}
Gdzie $g_*$ to rozwiązanie optymalne w poprzedniej generacji, $L(\lambda)$ to krok pochodzący z rozkładu Lévy'ego a  $\gamma$ to współczynnik skalujący. Wzór ten zakłada że poruszanie się organizmów zapylających może być modelowane poprzez loty Lévy'ego \cite{FPA}. 
Zapylanie lokalne modelowane jest przy pomocy wzoru \cite{FPA}:
\begin{equation}
x^{t+1}_i=x^t_i + \epsilon(x^t_j - x^t_k)\label{local}
\end{equation}
Gdzie $x^t_j$ i $x^t_k$ to inne wektory rozwiązań, a $\epsilon$ to rozmiar kroku który losowany jest z rozkładu jednorodnego w przedziale od 0 do 1. Niniejszy wzór modeluje stałość kwiatów (flower constancy), czyli zjawisko  w którym obserwuje się chęć organizmów zapylających do odwiedzania wyłącznie kwiatów tego samego gatunku. Zjawisko to ma uzasadnienie ewolucyjne zarówno dla kwiatów ( więcej zapylonych osobników) jak i dla organizmów, które dzięki dużej liczbie zapylonych kwiatów uzyskają dostęp do większej ilości nektaru \cite{FPA}. 
Wybór pomiędzy rodzajami zapylania kontrolowany jest przez prawdopodobieństwo przełączenia (ang. switch probability), faworyzujące zapylanie lokalne, którego sugerowana w literaturze wartość to 0.8. 
Przebieg algorytmu:
\begin{enumerate}
\item Wylosowanie początkowych wektorów rozwiązań.
\item Znalezienie rozwiązania optymalnego w obecnej generacji
\item Wybór typu zapylenia dla każdego rozwiązania zgodnie z prawdopodobieństwem przełączenia
\item Wyznaczenie nowych rozwiązań korzystając z odpowiedniego z wzorów \ref{global} i \ref{local} 
\item Wyznaczenie wartości optymalizowanej funkcji w nowych rozwiązaniach, aktualizacja rozwiązań jeżeli nowe rozwiązanie osiągnęło lepszy wynik
\item Powtórzenie kroków 2-5 przez pewną liczbę iteracji
\end{enumerate}
\newpage
\thispagestyle{empty}
\mbox{}

\newpage
%% =========================== ROZDZIAL 3 ================================
\section{Implementacja}

\subsection{Język programowania R}
\paragraph{}
W pracy  wykorzystano interpretowany język programowania R. R to język służący do przeprowadzania obliczeń i analiz statystycznych i wizualizacji ich wyników. Obecnie jest jednym z najczęściej wykorzystywanych języków w analizie danych i uczeniu maszynowym zaraz po Pythonie. Największą zaletą tego języka jest ogromna ilość dostępnych bibliotek i narzędzi, duża ilość algorytmów z dziedzin statystyki i analizy danych jest już zaimplementowana i dostępna w postaci pakietów. Kolejną zaletą języka R jest fakt że jest dostępny na otwartej licencji GNU. Ilość dostępnych bibliotek i wbudowanych funkcjonalności umożliwiają szybkie pisanie algorytmów, ich testowanie i prototypownie, jednakże niska wydajność języka i brak funkcjonalności związanych z bezpieczeństwem powodują że R nie nadaje się do budowy finalnych aplikacji. R jest językiem który znacznie lepiej sprawdzi się w pracy naukowej. Środowiskiem programistycznym jakie wykorzystano jest darmowe RStudio.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{dudel.jpg}
    \caption{Logo języka R}
\end{figure}
 \newpage
\subsection{Implementacja PNN}
\paragraph{}
Probabilistyczna sieć neuronowa została zrealizowana jako klasa referencyjna języka R. Taki zabieg ma na celu umożliwienie przechowywania stanu sieci, aby pozwolić na wielokrotne wykorzystanie gotowego modelu do klasyfikacji wielu elementów. Zaimplementowana sieć posiada możliwość wyboru następujących parametrów: 
\begin{itemize}
\item Parametr wygładzania:
\begin{enumerate}
\item W formie skalara, jeden parametr wygładzania dla wszystkich elementów.
\item W formie wektora o długości odpowiadającej ilości wymiarów, aby uzyskać inne wygładzanie dla każdego stopnia swobody.
\item W formie wektora o długości równej liczbie klas, aby zapewnić różne wygładzanie dla każdej klasy.
\item W formie macierzy aby uzyskać różne parametry dla każdej klasy i każdego wymiaru. 
\end{enumerate}
\item Jądro estymacji, wybierane jako argument znakowy z listy dostępnych jąder wymienionych w tabeli \ref{tbl:myLboro}
\item Siła modyfikacji parametru wygładzania. W przypadku podania siły 0 obliczenia związane z modyfikacją nie zostają wykonane. 
\end{itemize}
Oprócz wspomnianych parametrów do konstruktora sieci neuronowej przekazujemy również zbiór danych w postaci macierzy oraz wektor etykiet klas. Po inicjalizacji klasa reprezentująca sieć udostępnia metodę classify zwracającą etykiety klas umożliwiającą klasyfikację nowych danych. Funkcja classify jest zwektoryzowana, można przekazywać do niej całe wektory elementów do zaklasyfikowania, w takim wypadku zostanie zwrócony wektor etykiet klas do których przyporządkowano odpowiednie elementy. Wektoryzacja funkcji znacznie przyśpiesza proces klasyfikacji dla większej ilości elementów. Funkcja classify na każdym z elementów wywołuję funkcję get\_class\_score, a następnie wybiera etykietę klasy dla której zwrócony został najwyższy wynik. Funkcja get\_class\_score oblicza wartość estymatora gęstości prawdopodobieństwa stworzonego dla danej klasy w badanym punkcie. Wykorzystanie klasy do implementacji algorytmu umożliwia zmniejszenie ilości argumentów przekazywanych pomiędzy funkcjami, co optymalizuje ilość wykorzystywanej pamięci. 
\newpage
\begin{lstlisting}[caption={Kod funkcji classify i get\_class\_score}]
get_class_score = function(x, class_index) {
  kernel_density=function(cls, mod){
    univariate=kernel((x - cls)/(smooth[,class_index] * mod))
    return(colProds(univariate)/mod)
  }
  kernels = mapply(kernel_density,
                    data.frame(test_set[[class_index]]),
                    modification[[class_index]])
  m=ncol(test_set[[class_index]])
  return(rowSums2(to_mat(kernels))/(prod(smooth[,class_index]) * m))
}

classify = function(x) {
  x <- t(to_mat(x))
  class_scores =sapply(seq_along(test_set),
                       Curry(get_class_score, x = x))
  return(names(test_set)[max.col(to_mat(class_scores))])
}
\end{lstlisting}
Klasa PNN posiada również implementację algorytmu modyfikacji parametrów wygładzania. Jeżeli podana zostanie siła modyfikacji większa od zera, wykonane zostaną obliczenia współczynników modyfikacji dla każdego z elementów zbioru trenującego, a następnie zostaną one użyte do budowania estymatorów jądrowych poszczególnych klas.
\paragraph{}
Poza wspomnianymi funkcjonalnościami do wykorzystania z klasą PNN zaimplementowano funkcję pozwalającą na wyznaczenie parametrów wygłądzania metodą plug-in dla dowolnego jądra wymienionego w tabeli \ref{tbl:myLboro}. Wykorzystano trzystopniowy algorytm plug-in, do estymacji pochodnych funkcji gęstości wykorzystano jądro Cauchy'ego. Parametry wygładzania obliczane mogą zostać dla całego zbioru danych, bądź dla każdej z klas z osobna. 
\paragraph{}
\newpage
Dodatkowo w celu ułatwienia prezentacji wyników zaimplementowano funkcję umożliwiającą rysowanie wykresów estymatorów jądrowych wykorzystywanych w klasie PNN. Funkcja wyświetla trójwymiarowy przekrój estymatora dla dwóch zmiennych, a pozostałe zostają ustawioną na średnią wartość w obrębie klasy.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{dudel.jpg}
    \caption{Przykład wykresu przekroju estymatora jądrowego}
\end{figure}
\newpage
\subsection{Implementacja FPA}
Flower pollination  algorithm zaimplementowano w postaci funkcji przyjmującej funkcję do optymalizacji, wymiarowość optymalizowanej zmiennej, rozmiar populacji i maksymalną ilość iteracji które należy wykonać. Do generacji liczb losowych z rozkładu jednostajnego wykorzystano funkcje wbudowane, natomiast do generacji lotów Levy'ego wykorzystano algorytm Mantegna zaproponowany w \cite{FPA}. 
\begin{lstlisting}[caption={Algorytm Mantegana do generacli lotów Levy'ego}]
Levy <- function(d, l, scale) {
    sigma <- (gamma(1 + l) * sin(pi * l/2)
              /(gamma((1 + l)/2) * l * 2^((l - 1)/2)))^(1/l)
    u <- rnorm(d, sd = sigma)
    v <- rnorm(d)
    step <- u/(abs(v))^(1/l)
    return(scale * step)
}
\end{lstlisting}
Generacja odbywa się poprzez obliczenie ilorazu zmiennych pochodzących z dwóch rozkładów normalnych, u i v. Rozkład u ma odchylenie standardowe określone w parametrze sigma, natomiast rozkład v ma odchylenie standardowe równe 1. 
\newpage
\subsection{Integracja algorytmów}
\paragraph{}
Do oceny jakości klasyfikacji w niniejszej pracy wykorzystano dziesięciokrotną walidację krzyżową. W procesie tym zbiór danych dzielony jest na dziesięć równych części, a następnie dziesięciokrotnie przeprowadzana jest klasyfikacja w której jako zbiór treningowy wykorzystuje się dziewięć z dziesięciu części, i sprawdza się jakość klasyfikacji pozostałej jednej dziewiątej. Jakość klasyfikacji mierzona jest jako iloczyn ilości poprawnie zaklasyfikowanych elementów i liczności zbioru klasyfikowanego.  
\paragraph{}
FPA wymaga sformułowania funkcji celu, zaimplementowano wersję algorytmu służącą do maksymalizacji wartości funkcji. Algorytm optymalizacyjny użyty został do automatycznego dobierania wartości parametru wygładzania tak aby osiągnąć najlepszą jakość klasyfikacji. Stworzono więc funkcję której jedynym parametrem wejściowym jest macierz wygładzania, a parametrem wyjściowym jest wynik 10-cio krotnej walidacji krzyżowej procesu klasyfikacji zbioru danych. Optymalizacji można poddać wszystkie możliwości doboru zmienności parametru wygładzania.
\paragraph{}
FPA jest algorytmem optymalizacji globalnej przeznaczonym do pracy z funkcjami gdzie przestrzeń rozwiązań jest nieograniczona. W przypadku optymalizacji doboru parametrów wygładzania przestrzeń rozwiązań ograniczona jest od dołu przez zero, i teoretycznie nie jest ograniczona z góry. Bardzo wysokie wartości parametru wygładzania bardzo rzadko mają jednak zastosowanie praktycznie, dlatego też można stosować również ograniczenia odgórne. Należało więc zmodyfikować algorytm FPA w taki sposób aby umożliwić podanie wartości granicznych w obrębie których ma zostać przeprowadzona optymalizacja. Aby to osiągnąć rozszerzono sygnaturę funkcji o dwa dodatkowe parametry, ograniczono zakres losowania populacji początkowej tak aby wszystkie wyniki znalazły się wewnątrz ograniczonego przedziału. Kolejnym krokiem modyfikacji było dopasowanie procesu losowania nowych położeń w taki sposób aby zwracane były tylko nowe lokalizacje w obszarze optymalizowanym. Osiągnięto to poprzez powtarzanie losowania tak długo jak otrzymany wynik nie spełnia założonych kryteriów.  Powyższa modyfikacja znacznie poprawiła szybkość zbieżności algorytmu, odpowiednie dobranie granic optymalizacji pozwala znacznie przyśpieszyć proces. 
\paragraph{}
Ostatnią modyfikacją algorytmu w celu lepszej integracji z zadaniem optymalizacji parametrów wygładzania jest dodanie możliwości wprowadzenia znanego dobrego rozwiązania. Jeżeli chcemy optymalizować funkcję, ale mamy rozwiązanie pochodzące z innego algorytmu możemy podać to rozwiązanie do funkcji FPA, co zapewni że jedno z rozwiązań z populacji początkowej znajdzie się w tym punkcie. W przypadku optymalizacji parametrów wygładzania, jako znane dobre rozwiązanie można wykorzystać na przykład wynik metody plug-in.  Taka modyfikacja gwarantuje otrzymanie wyniku nie gorszego niż podany, jednakże może negatywnie wpłynąć na zdolności optymalizacyjne algorytmu. Wpływ modyfikacji zostanie zbadany w kolejnym rozdziale. 
\newpage
\thispagestyle{empty}
\mbox{}

\newpage
%% =========================== ROZDZIAL 4 ================================
\section{Analiza użyteczności}
Do oceny użyteczności algorytmu wykorzystana zostanie dziesięciokrotna walidacja krzyżowa na 4 standardowych zbiorach danych:
\begin{table}[H]
\centering
\caption{Zbiory danych}
\label{zbiory}
\begin{tabular}{|l|l|l|}
\hline
Nazwa         & Ilość elementów & Wymiarowość \\ \hline
Seeds   & 210 & 8 \\ \hline
Cardiotocography (CTG)   & 2126 & 35 \\ \hline
Breast Cancer Wisconsin  & 569 & 32 \\ \hline
Iris  & 150 & 5 \\ \hline
\end{tabular}
\end{table}
Ocenę będzie stanowić stosunek próbek poprawnie zaklasyfikowanych do liczności zbioru danych. 
\subsection{Badanie wpływu doboru parametrów}
\subsubsection{Wybór jądra estymacji}
Istnieje wiele jąder które można wykorzystać do estymacji jądrowej i klasyfikacji przy użyciu PNN, nie można jednoznacznie stwierdzić które jest najlepsze. Bardzo często wybór jądra zależy od zbioru danych, często jest też uwarunkowywany łatwością przeprowadzania obliczeń z wykorzystaniem danego jądra. W tabeli \ref{kernelchoice} przedstawiono wyniki klasyfikacji osiągnięte przez sieć przy użyciu wszystkich jąder z tabeli \ref{tbl:myLboro}. Do wykonania klasyfikacji wykorzystano stały parametr wygładzania wynoszący 1 dla wszystkich klas i wymiarów.

\begin{table}[H]
\begin{adjustwidth}{-.5in}{-.5in}  
\centering
\caption{Wybór jądra, h=1}
\label{kernelchoice}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
              & Normalne & Trójkątne & Cauchyego & Epanechinkowa & Jednostajne & Dwuwagowe\\ \hline
Seeds   & 0.9095238 & 0.8904762 & 0.9 & 0.8857143 & 0.8952381 & 0.8857143 \\ \hline
CTG     & 0.6622265 & 0.3283063 & 0.9388542 & 0.3344406 & 0.3494707 & 0.3551023 \\ \hline
Breast Cancer & 0.4920739 & 0.4974311 & 0.9402569 & 0.5184211 & 0.5007832 & 0.4955514 \\ \hline
Iris & 0.9266667 & 0.96 & 0.9533333 & 0.9666667 & 0.9133333 & 0.9666667\\ \hline
\end{tabular}
\end{adjustwidth}
\end{table}
\paragraph{}
Wyniki klasyfikacji w zbiorach o niskiej wymiarowości są zbliżone dla wszystkich jąder, natomiast w zbiorach o wyższej wymiarowości zauważalna jest znaczna różnica pomiędzy jądrami Cauchiego i Normalnym a pozostałymi. Te dwa jądra odróżniają się od pozostałych faktem nieskończonej szerokości. Wszystkie pozostałe jądra dla wartości o module większym od jedności przyjmują wartość zero. Dokładniejsza analiza wyników gęstości prawdopodobieństwa dla poszczególnych klas pokazuje że w przypadku jąder o skończonym czasie trwania większość badanych elementów uzyskuje gęstość prawdopodobieństwa równą zero. Zjawosko to wynika z klątwy wysokiej wymiarowości ( ang. curse of dimentinality). W przestrzeniach o dużej liczbie wymiarów uzyskanie dobrego pokrycia dostępnego miejsca wymaga ogromnej ilości elementów. W przypadku jąder o skończonej szerokości, niskie pokrycie skutkuje powstawaniem przerw w estymatorze. Jeżeli klasyfikowany element trafi na tego rodzaju przerwę, istnieje duże prawdopodobieństwo że nie zostanie poprawnie zaklasyfikowany. Jądra o nieskończonej szerokości, jak na przykład jądro cauchiego radzą sobie z tą sytuacją znacznie lepiej, ponieważ pokrywają one w teorii całą przestrzeń rozwiązań za każdym razem, dlatego też teoretycznie zawsze powinniśmy uzyskać niezerową wartość gęstości prawdopodobieństwa, na podstawie której można dokonać klasyfikacji. W praktyce ograniczona dokładność obliczeń komputerowych powoduje że nawet okna Cauchiego i normalne mają ograniczoną szerokość, jednakże jest ona nadal znacznie większa od szerokości okien skończonych. Próba rozwiązania problemu skończonej szerokości zostanie przeprowadzona w kolejnych podrozdziałach przy pomocy odpowiedniego dostrojenia parametru wygładzania. 
\paragraph{}
W prawie wszystkich przypadkach najlepiej sprawdza się okno Cauchiego. Teoretycznie najlepsze okno Epanechinkowa wypadło najlepiej tylko w jednym przypadku. Powyższy wynik jest pierwszą sugestią że stosowanie metod optymalnych asymptotycznie do estymacji rozkładu prawdopodobieństwa może nie być najlepszym rozwiązaniem do zastosowania w PNN.
\subsubsection{Stopnie swobody parametru wygładzania}
\paragraph{}
Parametr wygładzania można dobierać z różną dokładnością.  Można dobierać tylko jeden globalny parametr, jak również osobny parametr dla każdego wymiaru zbioru danych czy dla każdej klasy osobno. 
\paragraph{}
Stosowanie doboru osobnego parametru wygładzania dla każdego z wymiarów jest uzasadnione zróżnicowaniem zmienności danych. Bardzo często zdarza się że w jednym zbiorze danych znajdą się kolumny o odchyleniu standardowym różnych rzędów wielkości. W takiej sytuacji, gdyby dobrano wyłącznie jeden parametr wygładzania jeden z wymiarów musiałby być wy-estymowany niepoprawnie ze względu na zbyt duże lub zbyt małe wygładzenie. Przykład takiej sytuacji wykryto badając zbiór seeds wykorzystując okno Epanechnikowa, przy zastosowaniu parametru wygładzania 2 dla wszystkich wymiarów osiągnięto wynik 0.9047619, jednakże część wymiarów przy takim ustawieniu została za bardzo wygładzona i część informacji została utracona. Ustawienie wygładzania jednego z wymiarów na 0.5 przyniosło poprawę wyniku do 0.9142857. Metoda plug-in umożliwia dobór parametrów wygładzania dla każdego z wymiarów. 
\paragraph{}
Osobny parametr wygładzania dla każdej z klas również może być przydatny, można zaistnieć sytuacja w której w jednym zbiorze danych znajdą się klasy bardzo szerokie, charakteryzujące się dużą zmiennością wielu atrybutów jak i klasy bardzo specyficzne, o bardzo niskiej zmienności. W sytuacji kiedy używalibyśmy tylko jednego parametru wygładzania niezależnie od klasy, mogło by się zdarzyć tak że poprawne wygładzenie klasy szerokiej spowodowało by że dwie zbliżone do siebie klasy wąskie zostały by ze sobą połączone. Jednakże dobór osobnych parametrów wygładzania dla każdej z klas niesie ze sobą pewne problemy. Przede wszystkim, wartość parametru wygładzania wpływa na wartość estymowanej funkcji gęstości prawdopodobieństwa. Wysokie wygładzanie prowadzi do niskich wartości gęstości prawdopodobieństwa ponieważ zawsze zachowana musi zostać wartość całki gęstości prawdopodobieństwa. Taki wpływ może powodować błędną klasyfikację, elementy mogą być rzadziej klasyfikowane do klasy bardziej wygładzonej. Kolejnym problemem jest możliwość pojawienia się zerowego odchylenia standardowego. Może zdarzyć się tak że w obrębie klasy dana kolumna będzie miała stałą wartość, a w większości algorytmów wyznaczania parametru wygładzania wygładzanie jest odwrotnie proporcjonalne do odchylenia standardowego. Taka sytuacja prowadzi do wyznaczenia ogromnych bądź nieskończonych wartości parametru wygładzania, co zwykle uniemożliwia klasyfikację.
\newpage
\vfill 
\begin{figure}
    \centering
    \includegraphics[width=1.2\textwidth]{dudel.jpg}
    \caption{Estymator jądrowy, wygładzanie równe jeden dla każdego wymiaru i klasy}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.2\textwidth]{dudel.jpg}
    \caption{Estymator jądrowy, wygładzania 1.5 dla jednej z klas}
\end{figure}
\vfill
\clearpage
\newpage
\vfill 
\begin{figure}
    \centering
    \includegraphics[width=1.2\textwidth]{dudel.jpg}
    \caption{Estymator jądrowy, wygładzanie 2 dla jednego z wymiarów}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.2\textwidth]{dudel.jpg}
    \caption{Estymator jądrowy, wygładzanie 2 dla jednego z wymiarów i 1.5 dla jednej z klas}
\end{figure}
\vfill
\clearpage
\newpage
\paragraph{}
Należy również pamiętać że większa ilość parametrów wygładzania to również większa złożoność obliczeniowa. Algorytmy wyznaczania parametrów wygładzania mają zwykle wysokie wymagania obliczeniowe, metoda plug-in ma złożoność kwadratową. W przypadku optymalizacji przy użyciu FPA większa ilość parametrów do optymalizowania znacznie spowalnia proces optymalizacji. 
\begin{table}[H]
\centering
\caption{Wpływ ilości stopni swobody parametru wygładzania na proces optymalizacji FPA}
\label{swoboda}

\begin{tabular}{|l|l|l|l|}
\hline
\theadfont\diagbox[width=15em]{Wariant}{Ilość iteracji} & 10 & 20 & 100\\ \hline
Jeden parametr   & 0.9047 & 0.9047  &0.9047\\ \hline
Różne dla wymiarów  & 0.9523 & 0.9571  & 0.9619\\ \hline
Różne dla klas  & 0.895 & 0.9047  & 0.9142\\ \hline
Różne dla wymiarów i klas  & 0.9238 & 0.94761  &0.9571\\ \hline
\end{tabular}
\end{table}
\paragraph{}
Tabela \ref{swoboda} pokazuje że optymalizacja dla parametru wygładzania zmiennego dla każdego wymiaru daje najlepsze rezultaty najszybciej. Można podejrzewać, że optymalizacja dla każdego wymiaru i klasy dała by lepsze rezultaty, ale wymagało by do bardzo dużej ilości iteracji. Z powyższych przyczyn do dalszych rozważań wybrano wariant wyboru parametru wygładzania dla każdego wymiaru, lecz taki sam dla wszystkich klas. 

\subsubsection{Siła modyfikacji parametru wygładzania}
\paragraph{}
Literatura najczęściej podaje że dobrą wartością siły modyfikacji parametru wygładzania jest 0.5, w niniejszym podrozdziale zbadana zostanie zależność jakości klasyfikacji od wartości parametru siły modyfikacji. Ponadto podjęta zostanie decyzja czy algorytm FPA należy uruchamiać z już ustawionym wygładzaniem, czy wystarczy znaleźć optymalne parametry bez wygładzania. 
\paragraph{}
Pierwszym eksperymentem jaki przeprowadzono jest porównanie wyników uzyskanych przez klasyfikator z parametrem wygładzania wybranym przez algorytm FPA w poprzednim podrozdziale. Zastosowanie parametru modyfikacji spowodowało spadek wyniku z 0.9619 do 0.9285. Można więc wnioskować, że stosowanie parametru wygładzania po zastosowaniu FPA nie jest dobrym rozwiązaniem. 
Drugim przetestowanym podejściem byłą optymalizacja parametrów wygładzania z modyfikacją ustawioną na 0.5. Wadą takiego rozwiązania jest znacznie dłuższy czas wykonywania, ze względu na konieczność obliczenia parametrów modyfikacji dla każdego wektora rozwiązań. Rozwiązanie to również nie okazało się skuteczne, wynik jaki udało się osiągnąć w 100 iteracjach to tylko 0.9095. 
\begin{table}[H]
\centering
\caption{Wpływ siły modyfikacji na jakość klasyfikacji}
\label{modyfikacja}
\begin{tabular}{|l|l|l|l|}
\hline
\theadfont\diagbox[width=25em]{Wariant}{Siła wygładzania} & 0 & 0.5 & 1\\ \hline
FPA bez modyfikacji, Epanechnikow, seeds   & 0.9619 & 0.9047  &0.9047\\ \hline
plug-in, Cauchy, seeds   &0.9142 & 0.9142  &0.9142\\ \hline
h=1, Cauchy, seeds   &0.9 & 0.8952  &0.8952\\ \hline
plug-in, Cauchy, breast cancer   &0.9825 & 0.9825  & 0.9821\\ \hline

\end{tabular}
\end{table}
Wyniki zaprezentowane w tabeli \ref{modyfikacja} wskazują że stosowanie modyfikacji wygładzania ma neutralny lub negatywny wpływ na proces klasyfikacji. Oczywiście powyższy wniosek nie musi być prawdziwy dla innych zbiorów danych, jednakże w niniejszej pracy modyfikacja parametru wygładzania nie zostanie wykorzystana.
\subsubsection{Strojenie parametrów FPA}
\paragraph{}
Algorytm Flower Pollination posiada parametry wejściowe, które należy odpowiednio dobrać aby uzyskać najlepszą efektywność. Efektywność algorytmu wyraża się przez szybkość z jaką wynik zbiega do optymalnego, jak również zdolność algorytmu do odnajdywania ekstremów globalnych. W przypadku FPA w klasycznej implementacji zaproponowanej w \cite{FPA} do inicjalizacji algorytmu należy wybrać jedynie liczność populacji. Wybór ten ma bardzo duży wpływ na zbieżność algorytmu, czym większa populacja tym szybciej algorytm znajduje coraz lepsze rozwiązania, jednakże większa ilość elementów to również więcej obliczeń do wykonania.  Należy również pamiętać, że duża liczba elementów populacji daje większą szanse na znalezienie globalnego celu optymalizacji, niska liczba elementów może prowadzić do zatrzymania algorytmu w lokalnym ekstremum. Wysoka liczność populacji jest również pożądana w celu zwiększenie pokrycia badanej przestrzeni. Klątwa wymiarowości oznacza że przestrzenie o wysokich wymiarowościach bardzo trudno jest wypełnić, dlatego też zwłaszcza w przypadkach zbiorów o wysokiej wymiarowości istotny jest duży rozmiar populacji początkowej.
\paragraph{}
W celu doboru optymalnej wartości parametru liczności populacji przeprowadzono test w którym porównano jakość klasyfikacji i czas wykonywania algorytmu przy użyciu liczności 5 i 50 elementów.Test przeprowadzono wykorzystując optymalizację wyniku pięciokrotnej walidacji krzyżowej. Wykorzystując pięcioelementowa populacja po 100 generacjach osiągnęła wynik 0.9047, obliczenia zajęły 125 sekund. Populacja pięćdziesięciu elementów osiągnęła wynik 0.9619 a czas wykonywania wyniósł 1250 sekund. Różnica w czasie wykonywania jest zrozumiała, złożoność obliczeniowa FPA zależy liniowo od wielkości populacji. FPA wykorzystujące populację 50-ciu elementów osiągnęło wynik lepszy od przypadku pięcioelementowego już w pierwszej generacji, której obliczenie zajęło zaledwie 30 sekund. 


\paragraph{}
Kolejnym elementem mającym wpływ na zbieżność algorytmu FPA jest ograniczenie domeny poszukiwań. Z definicji FPA jest algorytmem optymalizacji globalnej funkcji o nieskończonej dziedzinie, jednakże po zastosowaniu modyfikacji opisanej w podrozdziale 3.4  można wykorzystać go do optymalizacji funkcji ograniczonych. Funkcja jakości klasyfikacji w zależności od wartości parametrów wygładzania jest funkcją ograniczoną, parametr wygładzania nie powinien mieć wartości mniejszej od zera, a wartości wygładzania rzędu dziesiątek czy setek tysięcy w praktyce nigdy nie są wykorzystywane. Dlatego też można ograniczać dziedzinę poszukiwań optymalnego parametru wygładzania. Przeprowadzono test wpływu modyfikacji na zbieżność algorytmu. Usunięcie ograniczeń spowodowało ogromne pogorszenie jakości klasyfikacji i zbieżności algorytmu. W poprzednim teście wykorzystano ograniczenie zakresu parametru wygładzania do przedziału $(0,10])$, po zmianie przedziału na $(0,10^6])$ wynik algorytmu w pierwszej iteracji spadł z 0.93 do 0.38, a finalny wynik wyniósł zaledwie 0.9095. 

\paragraph{}
Ostatnią możliwością modyfikacji jest wprowadzona możliwość podania punktu startowego do algorytmu FPA. W badanych zbiorach algorytm FPA niemalże natychmiast znajduje rozwiązanie lepsze od proponowanego przez metodę plug-in, dlatego też rozwiązanie to nie zostało wykorzystane przede wszystkim ze względu na wysoką złożoność obliczeniową metody plug-in. Jednakże, możliwość taka może być przydatna w innych zbiorach danych. Nie zaobserwowano żadnego wpływu stosowania tej modyfikacji w badanych zbiorach danych.
\subsubsection{Wpływ normalizacji na klasyfikację}
W pracy badane są zbiory danych z atrybutami o bardzo różnych charakterystykach zmienności co wymusza dopasowanie parametrów klasyfikacji mocno różniących się dla każdego wymiaru. Można analizować przykładowo dane ma temat wzrostu i na temat zarobków, obydwie zmienne będą miały zarówno różną wartość oczekiwaną jak i odchylenie standardowe.  Przeskalowanie zbioru danych w taki sposób aby uniknąć tego typu różnic mógłby teoretycznie uprościć analizę danych i procedury wyznaczania parametrów do algorytmów, jak na przykład parametr wygładzania. Przeprowadzono test sprawdzający wpływ zastosowania normalizacji przed przystąpieniem do analizy danych na wyniki klasyfikacji. 

\paragraph{}
Bardzo ważny problemem podczas wykorzystywania algorytmów optymalizacyjnych jest nadmierne dopasowanie danych. Zjawisko to wynika ze zbyt długiego treningu, z czasem optymalizowana struktura traci swoje możliwości generalizacji. Zbyt długi trening prowadzi do wyciągania wniosków z danych w sposób nie dający większego zrozumienia problemu, ale pozwalający na osiągnięcie lepszego wyniku. Aby rozwiązać ten problem należy tak dobrać ilość iteracji algorytmu aby uzyskać najbardziej zoptymalizowany wynik nie pozwalając na przetrenowanie. Aby to osiągnąć przeprowadzono testy wykorzystujące pięciokrotną walidację krzyżową. Należy przypomnieć, że algorytm FPA optymalizuje wynik pięciokrotnej walidacji krzyżowej, wykorzystane zostaną dwie walidacje krzyżowe, jedna w procesie tworzenia modelu z wykorzystaniem FPA, a druga do walidacji algorytmu tworzenia modeli. Przyczyną przetrenowania może być zarówno duża ilość iteracji jak i duża populacja. W celu doboru optymalnej ilości iteracji i wielkości populacji przeprowadzono porównanie, wyniki przedstawiono w tabeli  \ref{iteracje}.
\begin{table}[H]
\centering
\caption{Wpływ ilości iteracji i wielkości populacji na jakość klasyfikacji dla zbioru seeds.}
\label{iteracje}
\begin{tabular}{|l|l|l|l|}
\hline
\theadfont\diagbox[width=15em]{Liczność populacji}{Ilość iteracji} & 10 & 20 & 50\\ \hline
5  & 0.9285 & 0.9378  &0.9047\\ \hline
10 &0.9476 & 0.939  &0.941\\ \hline
20 &0.9333 & 0.8952  &0.8952\\ \hline
\end{tabular}
\end{table}

Najlepszym wynikiem jaki udało się uzyskać jest wynik 0.9476 dla przypadku dziesięciu iteracji i dziesięciu elementów populacji FPA. Takie ustawienia zostaną wykorzystane w niniejszej pracy, jednakże dla każdego zbioru danych wybór tych parametrów należy przeprowadzić osobno. 
\begin{table}[H]
\centering
\caption{Wpływ normalizacji na jakość klasyfikacji}
\label{modyfikacja}
\begin{tabular}{|l|l|l|l|}
\hline
\theadfont\diagbox[width=25em]{Wariant}{Siła wygładzania} & 0 & 0.5 & 1\\ \hline
FPA bez wygładzania, Epanechnikow, seeds   & 0.9619 & 0.9047  &0.9047\\ \hline
plug-in, Cauchy, seeds   &0.9142 & 0.9142  &0.9142\\ \hline
h=1, Cauchy, seeds   &0.9 & 0.8952  &0.8952\\ \hline
plug-in, Cauchy, breast cancer   &0.9825 & 0.9825  & 0.9821\\ \hline

\end{tabular}
\end{table}

Można zaobserwować że że wszystkich badanych przypadkach neutralizacja ma wpływ pozytywny lub neutralny na wyniki klasyfikacj, dlatego też zostanie użyta w finalnym algorytmie. Dodatkowo, zastosowanie normalizacji umożliwia określenie mniejszego rejonu optymalizacji dla algorytmu FPA, który będzie uniwersalny niezależnie od zbioru danych. Ogranicza to liczbę parametrów wejściowych algorytmu, i poprawia czas zbieżności.
\subsubsection{Wybór parametru wygładzania}
\paragraph{}
W niniejszej pracy zbadane zostaną dwie metody automatycznego doboru parametru wygładzania, metoda plug-in, oraz optymalizacji przy użyciu FPA. Dobierane będą osobne parametry wygładzania dla każdego z wymiarów. Do analizy wykorzystane zostanie jedno okno o nieskończonej szerokości ( Cauchiego) i jedno jądro o skończonej szerokości (Epanechinkowa).  Dokonano porównania jakości klasyfikacji dla przypadku wykorzystania stałego wygładzania, dla metody plug-in i dla algorytmu FPA uruchomionego na 100 generacji. 
\begin{table}[H]
\centering
\caption{Dobór parametrów wygładzania dla jądra Cauchiego}
\label{plugvsfpa}
\begin{tabular}{|l|l|l|l|}
\hline
          & 1 & plug-in & FPA\\ \hline
Seeds   & 0.9 & 0.9142 &0.9714 \\ \hline
Cardiotocography (CTG)   & 0.9388 & 0.9825 & 0.98824 \\ \hline
Breast Cancer Wisconsin  & 0.9402 & 0.94558 &0.9595\\ \hline
Iris  & 0.9533 & 0.9333 &0.97333\\ \hline
\end{tabular}
\end{table}
\paragraph{}
W tabeli \ref{plugvsfpa} można zauważyć że we wszystkich przypadkach algorytm FPA był w stanie osiągnąć lepsze wyniki niż metoda plug-in. Często już losowo wygenerowana populacja osiąga wynik znacznie lepszy od tego osiągniętego przez metodę plug-in. Dodatkowo w tabeli \ref{plugvsfpa2}obserwujemy że w przypadku użycia jąder o skończonej szerokości algorytm FPA świetnie radzi sobie z doborem wygładzania pozwalającego na dostateczne wypełnienie przestrzeni, i w przeciwieństwie do algorytmu plug-in umożliwia użycie jąder tego typu. Wynik taki jest kolejnym argumentem potwierdzającym tezę postulującą że stosowanie estymatora gęstości o najmniejszym błędzie średnio kwadratowym i w ogólności dążenie do jak najlepszego odwzorowania funkcji gęstości prawdopodobieństwa nie jest najbardziej optymalne dla zadania klasyfikacji przy użyciu PNN. Przykład jąder o skończonej szerokości pokazuje że dokładne pokrycie pewnych obszarów przestrzeni przez estymator jest ważniejsze niż błąd średnio kwadratowy. Kolejnym argumentem za skutecznością FPA jest możliwość optymalizacji oceniającej istotność konkretnych wymiarów w procesie klasyfikacji. Proces optymalizacji prowadzi do znalezienia takiego rozwiązania gdzie kolumny wprowadzające nieistotne bądź szkodliwe informacje mogą zostać wygładzone znacznie bardziej od pozostałych. Można zauważyć że różnica jakości klasyfikacji jest większa w małych i nisko-wymiarowych zbiorach danych. Fakt ten wynika z mniejszej przestrzeni rozwiązań którą łatwiej jest przeszukać. W tych przypadkach zastosowanie większej populacji i większej liczby iteracji mogło by dać jeszcze lepsze rezultaty.
\paragraph{}
Jeżeli klasyfikacja pojedynczego elementu zostanie uznana za operację jednostkową to złożoność obliczeniowa algorytmu plug-in jest kwadratowa względem ilości elementów w zbiorze danych, natomiast złożoność obliczeniowa algorytmu FPA jest liniowa. Jednakże, w przypadku algorytmu FPA liniowy czas wykonywania przemnożony zostaje przez stałą wynoszącą iloraz ilości iteracji i wielkości populacji początkowej. Dlatego też w mały zbiorach danych algorytm FPA wykonuje się znacznie wolniej niż algorytm plug-in, jednakże w zbiorach znacznie większych, jeżeli ilość iteracji i wielkość populacji pozostanie na tym samym poziomie algorytm FPA będzie wykonywał się szybciej. Zestawienie czasów wykonania przedstawiono w tabeli \ref{plugvsfpa3}.

\begin{table}[H]
\centering
\caption{Czas wykonania algorytmów}
\label{plugvsfpa3}
\begin{tabular}{|l|l|l|l|}
\hline
          & 1 & plug-in & FPA\\ \hline
Seeds   &  0.24 s & 1.12 s & 1250 s \\ \hline
Cardiotocography (CTG)   & 19 s & 336 s  & 89988 s\\ \hline
Breast Cancer Wisconsin  &1.95 s & 30.30 s &8793 s\\ \hline
Iris  & 0.18 s & 0.47 s &906.91 s\\ \hline
\end{tabular}
\end{table}
\paragraph{}
\subsubsection{Finalny algorytm}
\paragraph{}
Na podstawie wszystkich poprzednich obserwacji przygotowano algorytm służący do klasyfikacji który zostanie porównany z innymi dostępnymi metodami klasyfikacji w kolejnym podrozdziale. 
Założenia algorytmu:
\begin{itemize}
\item Wykorzystanie Probabilistycznej sieci neuronowej jako klasyfikatora
\item Dobór parametru wygładzania przy pomocy Flower Pollination Algorithm
\item Brak modyfikacji parametru wygładzania
\item Okno Cauchi'ego
\item Strojenie parametrów dla każdego wymiaru osobno, tak samo dla każdej z klas. 
\item Dokonano normalizacji zbioru danych przed podejściem do klasyfikacji
\end{itemize}
\subsection{Porównanie z istniejącymi algorytmami}
\paragraph{}
Do porównania z innymi algorytmami wykorzystana zostanie dziesięciokrotna walidacja krzyżowa. Porównane zostaną wyniki oraz czas wykonywania algorytmu dla wszystkich zbiorów danych wymienionych w tabeli \ref{zbiory}. Do porównania wykorzystaną popularne metody klasyfikacji:
\begin{itemize}
\item SVM - maszyna wektorów nośnych, jej działanie zostało opisane w podrozdziale 1.2.3.  Do porównania wyników wykorzystany zostanie pakiet  e1071 i wchodząca w jego skład funkcja svm. Parametry modelu cost i gamma zostały dobrane przy użyciu funkcji svm::tune, następnie stworzono model i przeprowadzono klasyfikację. 
\item Gradient Boosting (XGBoost) - algorytm polegający na tworzeniu modelu poprzez składanie mniej dokładnych modeli, najczęściej drzew decyzyjnych. Biblioteka XGBoost implementuje algorytm parallel tree boosting, umożliwiając rozwiązywanie problemów klasyfikacyjnych i regresyjnych. Biblioteka ta jest obecnie bardzo popularna w świecie analizy danych. Parametry modelu zostały dobrane eksperymentalnie. 
\item KNN - algorytm k najbliższych sąsiadów opisany w podrozdziale 1.2.1. Współczynnik k  
\end{itemize}

\begin{table}[H]
\centering
\caption{Czas wykonania algorytmów}
\label{plugvsfpa3}
\begin{tabular}{|l|l|l|l|l|}
\hline
          & SVM & XGBoost & kNN &FPA PNN\\ \hline
Seeds   &  0.9380 & 0.9333 & 0.9142 &\\ \hline
Cardiotocography (CTG)   & 0.9901 & 0.9901 & 0.9891 & \\ \hline
Breast Cancer Wisconsin  &0.9771 & 0.9666 & 0.97189 &\\ \hline
Iris  & 0.9466 & 0.94 & 0.96 & 0.9333\\ \hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Jakość klasyfikacji algorytmów}
\label{plugvsfpa3}
\begin{tabular}{|l|l|l|l|l|}
\hline
          & SVM & XGBoost & kNN &FPA PNN\\ \hline
Seeds   &  24.28 & 2.92 & 14.82 &\\ \hline
Cardiotocography (CTG)   & 3764 & 8.30 & 121 & \\ \hline
Breast Cancer Wisconsin  &188.4 &  3.16  & 20.43 &\\ \hline
Iris  & 12.73  & 2.22 & 12.75 &\\ \hline
\end{tabular}
\end{table}
\newpage
\thispagestyle{empty}
\mbox{}


\newpage
%% =========================== PODSUMOWANIE ==============================
\section{Podsumowanie}
\paragraph{}
asd
\newline
\newpage
\thispagestyle{empty}
\mbox{}

\newpage
%% =========================== ROZDZIAL 5 ================================
\section{Kierunki dalszego rozwoju}
\paragraph{}
Niniejsza praca stanowi jedynie badanie użyteczności algorytmu FPA do strojenia probabilistycznych sieci neuronowych, jednakże analizowana dziedzina jest niezwykle szeroka i istnieje jeszcze wiele elementów które należało by zbadać. Dużym problemem prezentowanego algorytmu jest jego czas wykonywania.  Przede wszystkim, niniejszy algorytm zaimplementowano w języku R, który świetnie nadaje się do szybkiego prototypowania, jednakże jest znacznie wolniejszy od języków niższego poziomu. Implementacja zarówno FPA jak i PNN w kompilowanym języku jak na przykład C++ przyniosłaby znaczny zysk wydajnościowy. Wartym rozważenia było by również przeniesienie obliczeń na GPU. Należało by również przetestować proponowane modyfikacje algorytmu PNN zakładające zastąpienie umieszczania wszystkich elementów zbioru danych w strukturze sieci umieszczaniem jedynie reprezentantów klastrów. Zwiększenie szybkości wykonywania algorytmu pozwoliło by na wykorzystanie optymalizacji wygładzania dla każdej z klas z osobna, jak również na zwiększenie populacji i liczby iteracji. Dopracowanie samego algorytmu mogło by polegać na zbadaniu innych metod przeszukiwania przestrzeni w celach optymalizacyjnych. Można również przetestować możliwości strojenia parametru modyfikacji wygładzania przy użyciu FPA przy jednoczesnym strojeniu pozostałych parametrów. Zainteresowanie sieciami typu PNN jest obecnie bardzo duże, dlatego też powstaje wiele algorytmów mających na celu poprawę architektury sieci tego typu. Istnieją modyfikację wprowadzające dodatkowe wagi i/lub warstwy, warto było by przeprowadzić próbę wykorzystania algorytmu FPA do optymalizacji wszystkich parametrów takich zmodyfikowanych sieci.

\newpage
\thispagestyle{empty}
\mbox{}

\newpage
%% =========================== BIBLIOGRAFIA ==============================
\section*{ }
\addcontentsline{toc}{section}{Bibliografia}
\markboth{Bibliografia}{}
\bibliographystyle{plain}
\renewcommand{\refname}{Bibliografia}
\begin{thebibliography}{}
\bibitem{knn}
Kramer O. (2013) K-Nearest Neighbors. In: Dimensionality Reduction with Unsupervised Nearest Neighbors. Intelligent Systems Reference Library, vol 51. Springer, Berlin, Heidelberg
\bibitem{tree}
Berk R.A. (2008) Classification and Regression Trees (CART). In: Statistical Learning from a Regression Perspective. Springer Series in Statistics. Springer, New York, NY
\bibitem{svm}
Cortes, C. Vapnik, V. Mach Learn (1995) 20: 273. \url{https://doi.org/10.1007/BF00994018}
\bibitem{nn}
Evans J.P. (1990) Classifying Artificial Neural Network Architecture. In: International Neural Network Conference. Springer, Dordrecht
\bibitem{kde}
Kulczycki P. (2005) Estymatory jądrowe w analizie systemowej. WNT, Warszawa
\bibitem{kde2}
Gramacki A. (2018) Kernel Density Estimation. In: Nonparametric Kernel Density Estimation and Its Computational Aspects. Studies in Big Data, vol 37. Springer, Cham
\bibitem{pnn}
Kowalski, P.A. Kulczycki, P. Neural Comput  Applic (2017) 28: 817. \url{https://doi.org/10.1007/s00521-015-2109-3}
\bibitem{epa}
Epanechnikov, V.A. (1969). "Non-parametric estimation of a multivariate probability density". Theory of Probability and its Applications. 14: 153–158.
\bibitem{FPA}
  Xin-She Yang,
  \emph{Nature-Inspired Optimization Algorithms},
  Rozdział 11 – Flower Pollination Algorithms
  2014.

\bibitem{R_webpage}
  Oficjalna dokumentacja języka R,
  \emph{\url{https://www.r-project.org/}}, ostatni dostęp: 03.05.2018
  

\end{thebibliography}

\newpage
%% =========================== SPIS ILUSTRACJI ===========================
\section*{ }
\listoffigures
\newpage
\addcontentsline{toc}{section}{Spis rysunków}

\newpage
%% =========================== SPIS TABEL ===========================
\section*{ }
\listoftables
\newpage
\addcontentsline{toc}{section}{Spis tabel}

\newpage
%% =========================== SPIS LISTINGÓW ===========================
\section*{ }
\renewcommand{\lstlistlistingname}{Spis listingów}
\lstlistoflistings 
\newpage
\addcontentsline{toc}{section}{Spis listingów}



\newpage
%% =========================== DODATKI ===========================
\section*{Dodatki}
\addcontentsline{toc}{section}{Dodatki}
\markboth{Dodatki}{}
\paragraph{}

\end{document}